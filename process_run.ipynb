{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "285e7773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import list\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from transformers import BertModel, DistilBertTokenizer, DistilBertModel\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AlbertModel\n",
    "\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "from transformers import XLNetTokenizer, XLNetModel\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "## 오늘 날짜 확인\n",
    "import datetime as dtime\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "## SSH 연결 \n",
    "import paramiko\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d71b2b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.25.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 여기서부터는 input 처리 부분! 추후에 함수화 할 것! 모듈화! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5855895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_path = 'google/electra-base-discriminator'  # \"monologg/koelectra-base-v3-discriminator\"\n",
    "model =   ElectraModel.from_pretrained(model_path)  # KoELECTRA-Small-v3\n",
    "tokenizer =  ElectraTokenizer.from_pretrained(model_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff892aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/saved_weights_2303_acc96(allagree).pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 저장한 모델 가중치 불러오기 \n",
    "device = torch.device('cpu')   # GPU 없을시, CPU로 해야함 단점은 속도가 느림!\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "#model = HalfAgreeClassifier()\n",
    "model = AllAgreeClassifier()\n",
    "\n",
    "out_dir = './model' \n",
    "## saved_weights_220629.pt  5 epoch 모델 \n",
    "#load weights of best model\n",
    "#path = out_dir+ '/' + 'saved_weights_2303_acc86(50agree).pt'\n",
    "path = out_dir+ '/' + 'saved_weights_2303_acc96(allagree).pt'\n",
    "print(path)\n",
    "model.load_state_dict(torch.load(path))  #, map_location=device   / strict = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1357b219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c90cca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bascrap/data/stock/investing_news/20230323/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model parameter\n",
    "MAX_LEN = 64\n",
    "batch_size = 16\n",
    "\n",
    "# ssh info\n",
    "host  = '121.254.150.83'\n",
    "port_num  = 6502\n",
    "user_id = 'bascrap'\n",
    "user_password = 'qwe123!@#'\n",
    "\n",
    "# target date \n",
    "yesterday = datetime.today() - timedelta(1)\n",
    "yesterday_date = yesterday.strftime(\"%Y%m%d\")\n",
    "## 오전 10 ~ 2시 사이에 업데이트? \n",
    "yesterday_date = '20230323'  # 테스트용 \n",
    "\n",
    "# ssh 타겟 경로 \n",
    "dir_path = '/home/bascrap/data/stock/investing_news/'\n",
    "folder_path = yesterday_date + '/'\n",
    "remote_dir_path = dir_path + folder_path \n",
    "remote_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6d397f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# main run \n",
    "target_list, ssh = ssh_access_day_files(host, port_num, user_id, user_password, remote_dir_path)\n",
    "result_list = sentiments_score_extract(target_list, remote_dir_path, ssh, tokenizer, MAX_LEN, batch_size)\n",
    "\n",
    "\n",
    "## 종목, 일짜 합산 평균 \n",
    "stock_dic = stock_dic_make(result_list)\n",
    "ticker_df = ticker_output(stock_dic)  # 종목 출력 \n",
    "day_avg_preds = day_output(stock_dic)  #하루 출력 \n",
    "day_df = yesterday, day_avg_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e565546",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198, 198)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 전체 출력 및 확인용 \n",
    "len(result_list), len(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cce1426f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>sentiment score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>0.225275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABB</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACN</td>\n",
       "      <td>0.393939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADBE</td>\n",
       "      <td>0.294118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>WFC</td>\n",
       "      <td>-0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>WMT</td>\n",
       "      <td>0.104167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>XL</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>XOM</td>\n",
       "      <td>-0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>X</td>\n",
       "      <td>0.137931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    symbol  sentiment score\n",
       "0     AAPL         0.000000\n",
       "1     ABBV         0.225275\n",
       "2      ABB         1.000000\n",
       "3      ACN         0.393939\n",
       "4     ADBE         0.294118\n",
       "..     ...              ...\n",
       "128    WFC        -0.071429\n",
       "129    WMT         0.104167\n",
       "130     XL        -0.500000\n",
       "131    XOM        -0.428571\n",
       "132      X         0.137931\n",
       "\n",
       "[133 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bef6ddf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.datetime(2023, 3, 29, 10, 38, 20, 680936), 0.09332147310540403)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5ea63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751cf626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61ca880f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>sentiment score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABB</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol  sentiment score\n",
       "2    ABB              1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol_input =  'ABB'\n",
    "on_symbol = ticker_df[ticker_df['symbol'] == symbol_input]\n",
    "on_symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30359af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae8c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e98d3525",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 후속 작업, 라이브러리를 활용해서 일짜별 종목 주가 불러오기 : 모델의 진짜 성능 평가 \n",
    "from pandas_datareader import data as pdr\n",
    "import yfinance as yf\n",
    "yf.pdr_override()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0feb5de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-03-20</th>\n",
       "      <td>32.380001</td>\n",
       "      <td>32.759998</td>\n",
       "      <td>32.320000</td>\n",
       "      <td>32.560001</td>\n",
       "      <td>31.646332</td>\n",
       "      <td>1150800</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-21</th>\n",
       "      <td>33.209999</td>\n",
       "      <td>33.369999</td>\n",
       "      <td>33.099998</td>\n",
       "      <td>33.270000</td>\n",
       "      <td>32.336407</td>\n",
       "      <td>1233200</td>\n",
       "      <td>0.021806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-22</th>\n",
       "      <td>33.549999</td>\n",
       "      <td>33.779999</td>\n",
       "      <td>33.119999</td>\n",
       "      <td>33.119999</td>\n",
       "      <td>32.190617</td>\n",
       "      <td>1487700</td>\n",
       "      <td>-0.004509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-23</th>\n",
       "      <td>33.509998</td>\n",
       "      <td>33.750000</td>\n",
       "      <td>33.169998</td>\n",
       "      <td>33.380001</td>\n",
       "      <td>32.443321</td>\n",
       "      <td>2115500</td>\n",
       "      <td>0.007850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-24</th>\n",
       "      <td>32.509998</td>\n",
       "      <td>32.770000</td>\n",
       "      <td>32.169998</td>\n",
       "      <td>32.750000</td>\n",
       "      <td>31.830999</td>\n",
       "      <td>2147000</td>\n",
       "      <td>-0.018874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-27</th>\n",
       "      <td>32.160000</td>\n",
       "      <td>32.299999</td>\n",
       "      <td>32.029999</td>\n",
       "      <td>32.139999</td>\n",
       "      <td>32.139999</td>\n",
       "      <td>1235800</td>\n",
       "      <td>-0.018626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close   Volume  \\\n",
       "Date                                                                         \n",
       "2023-03-20  32.380001  32.759998  32.320000  32.560001  31.646332  1150800   \n",
       "2023-03-21  33.209999  33.369999  33.099998  33.270000  32.336407  1233200   \n",
       "2023-03-22  33.549999  33.779999  33.119999  33.119999  32.190617  1487700   \n",
       "2023-03-23  33.509998  33.750000  33.169998  33.380001  32.443321  2115500   \n",
       "2023-03-24  32.509998  32.770000  32.169998  32.750000  31.830999  2147000   \n",
       "2023-03-27  32.160000  32.299999  32.029999  32.139999  32.139999  1235800   \n",
       "\n",
       "             Returns  \n",
       "Date                  \n",
       "2023-03-20       NaN  \n",
       "2023-03-21  0.021806  \n",
       "2023-03-22 -0.004509  \n",
       "2023-03-23  0.007850  \n",
       "2023-03-24 -0.018874  \n",
       "2023-03-27 -0.018626  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예시\n",
    "stock_df = pdr.get_data_yahoo(symbol_input, \"2023-03-19\", \"2023-03-28\")\n",
    "# 'Close' 컬럼을 이용하여 일별 변동률을 계산합니다.\n",
    "daily_returns = stock_df['Close'].pct_change()\n",
    "# 계산된 일별 변동률을 'Returns' 컬럼으로 추가합니다.\n",
    "stock_df['Returns'] = daily_returns\n",
    "# 결과를 출력합니다.\n",
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d2cfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9885700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0903b69e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343af118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f91add08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiments_score_extract(target_list, remote_dir_path, ssh, tokenizer, MAX_LEN, batch_size):\n",
    "    result_list = []\n",
    "\n",
    "    for number in range(len(target_list)):\n",
    "        symbol, title =  target_list[number].split('_')[0], target_list[number].split('_')[1]\n",
    "        remote_file_path = remote_dir_path +  target_list[number]\n",
    "        body = content_extract(remote_file_path)\n",
    "        tokenized_text = sent_tokenize(body)\n",
    "        text_list = [title] + tokenized_text\n",
    "        doc_tokens = preprocessing_contents(text_list)\n",
    "        preds = prediction_labels(doc_tokens, batch_size, tokenizer)\n",
    "        sum_preds = sum(preds)\n",
    "        avg_preds = sum(preds)/ len(preds)\n",
    "        # 종목, 제목, 본문, 문장별 예측라벨, 예측 합 , 예측 평균 , 날짜\n",
    "        data = [symbol, title, body, preds ,sum_preds, avg_preds ,yesterday_date ]\n",
    "        result_list.append(data)\n",
    "    \n",
    "    ssh.close()\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b195d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssh 이츠에이 수집한 investing.news 해외 종목 뉴스 접근 및 파일 리스트 추출\n",
    "def ssh_access_day_files(host, port_num, user_id, user_password, remote_dir_path):\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    ssh.connect(hostname=host, port=port_num, username=user_id, password=user_password)\n",
    "\n",
    "    stdin, stdout, stderr = ssh.exec_command(f'ls {remote_dir_path}')\n",
    "    file_list = stdout.read()\n",
    "\n",
    "    target_list = file_list.decode().split('\\n')\n",
    "    target_list = target_list[:-1]\n",
    "\n",
    "    return target_list, ssh\n",
    "\n",
    "\n",
    "# 파일 속 텍스트의 내용 추출 \n",
    "def content_extract(remote_file_path):\n",
    "    sftp = ssh.open_sftp()\n",
    "    with sftp.open(remote_file_path) as f:\n",
    "        content = f.read().decode()\n",
    "        \n",
    "    content_split = content.split('\\n\\n')\n",
    "\n",
    "    body = ''\n",
    "\n",
    "    for i in range(len(content_split)):\n",
    "        if len(content_split[i]) > 20 :\n",
    "            body = content_split[i]    \n",
    "    body = body.replace('\\n','')\n",
    "    body = body.split('Sources:')[0].strip()\n",
    "    sftp.close()        \n",
    "    return body\n",
    "\n",
    "\n",
    "\n",
    "# 모델 입력전 전처리 함수 \n",
    "def preprocessing_contents(text_list):\n",
    "\n",
    "    doc_tokens = []\n",
    "\n",
    "    for d in range(len(text_list)):\n",
    "        X_pred =  text_list[d]\n",
    "        X_pred = text_preprocessing(X_pred)\n",
    "        X_pred_split = X_pred.split(' ')\n",
    "        X_pred = text_word_one_limit(X_pred_split)\n",
    "        doc_tokens.append(X_pred)\n",
    "        \n",
    "    return doc_tokens\n",
    "\n",
    "# 모델에 예측 라벨을 계산하는 함수 \n",
    "def prediction_labels(doc_tokens, batch_size, tokenizer):\n",
    "    pred_inputs, pred_masks = preprocessing_for_bert(doc_tokens, tokenizer)\n",
    "    pred_dataset = TensorDataset(pred_inputs, pred_masks)\n",
    "    pred_sampler = SequentialSampler(pred_dataset)\n",
    "    pred_dataloader = DataLoader(pred_dataset, sampler=pred_sampler, batch_size= batch_size)\n",
    "\n",
    "    probs = bert_predict(model, pred_dataloader)  \n",
    "    preds = np.argmax(probs, axis = 1)\n",
    "\n",
    "    preds = list(preds)\n",
    "\n",
    "    results  = []\n",
    "\n",
    "    for p in range(len(preds)):\n",
    "        p_plus = preds[p]\n",
    "        p_real = p_plus - 1\n",
    "        results.append(p_real)\n",
    "        \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# output 을 위한 딕셔너리 구성\n",
    "def stock_dic_make(result_list):\n",
    "    stock_dic = {}\n",
    "    for r in range(len(result_list)):\n",
    "        in_data = result_list[r]\n",
    "        symbol, avg_preds = in_data[0], in_data[5]\n",
    "    \n",
    "        try: \n",
    "            stock_dic[symbol].append(avg_preds)\n",
    "        except: #  처음꺼라면 \n",
    "            stock_dic[symbol] = []\n",
    "            stock_dic[symbol].append(avg_preds)\n",
    "    return stock_dic\n",
    "\n",
    "\n",
    "## 종목별 감정 스코어 평균\n",
    "def ticker_output(stock_dic):\n",
    "    ## output \n",
    "    ticker_list = list(stock_dic.keys()) \n",
    "\n",
    "    ticker_data = []\n",
    "\n",
    "    for tn in range(len(ticker_list)):\n",
    "        ticker_name = ticker_list[tn]\n",
    "        stock_avg_preds = sum(stock_dic[ticker_name])/len(stock_dic[ticker_name])\n",
    "    \n",
    "        in_ticker = [ticker_name, stock_avg_preds]\n",
    "        ticker_data.append(in_ticker)\n",
    "\n",
    "    ticker_df = pd.DataFrame(ticker_data, columns = ['symbol', 'sentiment score'])\n",
    "    return ticker_df\n",
    "\n",
    "## 일짜 전체 출력 : 여기까지 하고 잠깐 스터디!\n",
    "def day_output(stock_dic):\n",
    "    ticker_list = list(stock_dic.keys()) \n",
    "\n",
    "    total_day = []\n",
    "\n",
    "    for t in range(len(ticker_list)):\n",
    "        ticker = ticker_list[t]\n",
    "        ticker_avg_preds = sum(stock_dic[ticker])/len(stock_dic[ticker])\n",
    "        total_day.append(ticker_avg_preds)\n",
    "    \n",
    "    day_avg_preds = sum(total_day) / len(ticker_list)\n",
    "    \n",
    "    return day_avg_preds    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78acc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9be83a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bbd1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e13b29b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 함수들 \n",
    "def text_word_one_limit(t_split):\n",
    "\n",
    "    con_text = ''\n",
    "    for tn in range(len(t_split)):\n",
    "        tns = t_split[tn]\n",
    "        if len(tns) <= 1 :\n",
    "            continue\n",
    "\n",
    "        con_text = con_text + ' ' + tns  \n",
    "    return con_text.strip()\n",
    "\n",
    "# 전처리 함수, 추후 증권 뉴스에 맞게 일부 추가 및 수정 필요 \n",
    "def text_preprocessing(text):\n",
    "    cleaned_text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    cleaned_text = re.sub('\\n', '', cleaned_text)\n",
    "    cleaned_text = re.sub('\\xa0', '', cleaned_text) \n",
    "    cleaned_text = re.sub(\n",
    "        '[\\{\\}\\[\\]\\/?;:|\\)…－〕.〔ⓘㅇ÷♠♣＜＞©◀Ⅱ·―Ⅱ＆,？☏☎™×『』《》／┌─┬┐│├ ┼┤└┴┘★〈●○[］〉±▨→↑↓∼％「」※ㆍ♥①②③④⑤⑥⑦⑧⑨△◇ ㈜ⓝ◈；：“”‘’ *~【】♡♥▽▷ⓒ▣◇□㈜◆☞■▶▲▼`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',\n",
    "        ' ', cleaned_text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "## 헤드 테일 짜르기 \n",
    "def truncation_method(df, head, tail):  # 문서 리스트, head 자를 비율, tail 자를 비율 \n",
    "    target_dt = []\n",
    "    #head\n",
    "    head_n = int(510 * head) \n",
    "\n",
    "    #tail\n",
    "    tail_n = int(510 * tail) \n",
    "    \n",
    "    print(head_n, tail_n)\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        target = df['text'][i]\n",
    "\n",
    "        target_head = target[:head_n]  # head 짜르기 \n",
    "        \n",
    "        if len(target) > 510  : # 510 max_len\n",
    "            tail = len(target) - tail_n # 510\n",
    "            target_tail = target[tail:]\n",
    "            text = target_head + target_tail \n",
    "        else:\n",
    "            #tail = 0\n",
    "            text = target\n",
    "        label = df['label'][i]\n",
    "        date = df['date'][i]\n",
    "        target_dt.append([label, text, date])\n",
    "\n",
    "    return target_dt\n",
    "\n",
    "def preprocessing_for_bert(data, tokenizer):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for sent in data:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sent,  \n",
    "            add_special_tokens=True,       \n",
    "            max_length=MAX_LEN,                \n",
    "            pad_to_max_length=True,              \n",
    "            return_attention_mask=True,    \n",
    "            truncation= True\n",
    "            )\n",
    "        \n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "\n",
    "# /model/saved_weights_2303_acc86(50agree).pt\n",
    "class HalfAgreeClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, freeze_bert=False):\n",
    "\n",
    "        super(HalfAgreeClassifier, self).__init__()\n",
    "\n",
    "        D_in, H, D_out = 768, 256, 3\n",
    "        model_path = 'google/electra-base-discriminator'\n",
    "        self.bert = ElectraModel.from_pretrained(model_path) \n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25), \n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    \n",
    "# ./model/saved_weights_2303_acc96(allagree).pt\n",
    "class AllAgreeClassifier(nn.Module):\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        super(AllAgreeClassifier, self).__init__()\n",
    "        D_in, H, D_out = 768, 768, 3\n",
    "        model_path = 'google/electra-base-discriminator'\n",
    "        self.bert = ElectraModel.from_pretrained(model_path) \n",
    "\n",
    "        self.classifier = nn.Sequential( \n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2), \n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "        \n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "# ======================================================================\n",
    "\n",
    "    \n",
    "def bert_predict(model, test_dataloader):\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs\n",
    "\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f281f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
